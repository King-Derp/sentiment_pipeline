"""add sentiment_score to sentiment_results

Revision ID: 6694fbaad3c6
Revises: 84df3802d234
Create Date: 2025-06-24 17:17:46.529689

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = '6694fbaad3c6'
down_revision: Union[str, None] = '84df3802d234'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Incremental upgrade: add JSON score column to `sentiment_results` and
    new aggregation columns to `sentiment_metrics`.  The migration is written
    defensively so that re-running (or running against a DB that already has
    the columns) is a no-op.
    """
    bind = op.get_bind()
    inspector = sa.inspect(bind)

    # ------------------------------------------------------------------
    # sentiment_results: add sentiment_scores_json
    # ------------------------------------------------------------------
    sr_columns = {c["name"] for c in inspector.get_columns("sentiment_results")}
    if "sentiment_scores_json" not in sr_columns:
        op.add_column(
            "sentiment_results",
            sa.Column(
                "sentiment_scores_json",
                postgresql.JSONB(astext_type=sa.Text()),
                nullable=True,
                comment="JSON object of scores for all sentiment classes.",
            ),
        )

    # ------------------------------------------------------------------
    # sentiment_metrics: add new aggregation columns
    # ------------------------------------------------------------------
    sm_columns = {c["name"] for c in inspector.get_columns("sentiment_metrics")}

    def _add_column_if_missing(col_name: str, column_obj: sa.Column) -> None:
        if col_name not in sm_columns:
            op.add_column("sentiment_metrics", column_obj)

    _add_column_if_missing(
        "time_bucket",
        sa.Column(
            "time_bucket",
            sa.TIMESTAMP(timezone=True),
            nullable=True,  # set NOT NULL after back-fill
            comment="Start of the time bucket.",
        ),
    )
    _add_column_if_missing(
        "source",
        sa.Column("source", sa.Text(), nullable=True, comment="Origin of the data."),
    )
    _add_column_if_missing(
        "source_id",
        sa.Column(
            "source_id",
            sa.Text(),
            nullable=True,
            comment="Secondary identifier from the source (use 'unknown' if none).",
        ),
    )
    _add_column_if_missing(
        "label",
        sa.Column("label", sa.Text(), nullable=True, comment="Categorical sentiment label."),
    )
    _add_column_if_missing(
        "count",
        sa.Column("count", sa.Integer(), nullable=True, comment="Number of sentiment results in bucket."),
    )
    _add_column_if_missing(
        "avg_score",
        sa.Column("avg_score", sa.Float(), nullable=True, comment="Average sentiment score for bucket."),
    )

    # Back-fill time_bucket from metric_timestamp if that column exists and new
    # time_bucket column was just created.
    if "time_bucket" not in sm_columns and "metric_timestamp" in sm_columns:
        op.execute(
            "UPDATE sentiment_metrics SET time_bucket = metric_timestamp "
            "WHERE time_bucket IS NULL;"
        )

    # Make the new columns NOT NULL where appropriate (only after potential back-fill)
    op.alter_column("sentiment_metrics", "time_bucket", nullable=False)
    op.alter_column("sentiment_metrics", "source", nullable=False)
    op.alter_column("sentiment_metrics", "source_id", nullable=False)
    op.alter_column("sentiment_metrics", "label", nullable=False)
    op.alter_column("sentiment_metrics", "count", nullable=False)
    op.alter_column("sentiment_metrics", "avg_score", nullable=False)

    # Create helpful composite index if it does not yet exist
    op.create_index(
        "idx_sentiment_metric_src_bucket_label",
        "sentiment_metrics",
        ["source", "time_bucket", "label"],
        unique=False,
        postgresql_where=None,
        if_not_exists=True,  # Alembic >=1.12 supports this flag
    )

    # Ensure sentiment_metrics remains / becomes a hypertable on the new column.
    # The `if_not_exists` clause prevents failure when already converted or if
    # TimescaleDB extension is unavailable in other environments.
    op.execute(
        "SELECT create_hypertable('sentiment_metrics', 'time_bucket', if_not_exists => TRUE);"
    )

    # ------------------------------------------------------------------
    # End of upgrade
    # ------------------------------------------------------------------
    return  # prevent execution of legacy autogenerated commands below

    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_table('sentiment_scores')
    op.drop_index(op.f('ix_raw_events_occurred_at'), table_name='raw_events')
    op.drop_index(op.f('ix_raw_events_processed_occurred_at'), table_name='raw_events', postgresql_where='(processed = false)')
    op.drop_index(op.f('ix_raw_events_source_source_id'), table_name='raw_events')
    op.drop_table('raw_events')
    op.add_column('sentiment_metrics', sa.Column('time_bucket', sa.TIMESTAMP(timezone=True), nullable=False, comment='Start of the time bucket.'))
    op.add_column('sentiment_metrics', sa.Column('source', sa.Text(), nullable=False, comment='Origin of the data.'))
    op.add_column('sentiment_metrics', sa.Column('source_id', sa.Text(), nullable=False, comment="Secondary identifier from the source (use 'unknown' if none)."))
    op.add_column('sentiment_metrics', sa.Column('label', sa.Text(), nullable=False, comment='Categorical sentiment label.'))
    op.add_column('sentiment_metrics', sa.Column('count', sa.Integer(), nullable=False, comment='Number of sentiment results in this bucket/category.'))
    op.add_column('sentiment_metrics', sa.Column('avg_score', sa.Float(), nullable=False, comment='Average sentiment score for this bucket/category.'))
    op.drop_index(op.f('idx_sm_source_component'), table_name='sentiment_metrics')
    op.drop_index(op.f('idx_sm_tags'), table_name='sentiment_metrics', postgresql_using='gin')
    op.drop_index(op.f('sentiment_metrics_metric_timestamp_idx'), table_name='sentiment_metrics')
    op.drop_column('sentiment_metrics', 'metric_value_float')
    op.drop_column('sentiment_metrics', 'metric_value_json')
    op.drop_column('sentiment_metrics', 'metric_timestamp')
    op.drop_column('sentiment_metrics', 'source_component')
    op.drop_column('sentiment_metrics', 'tags')
    op.drop_column('sentiment_metrics', 'metric_name')
    op.drop_column('sentiment_metrics', 'metric_value_int')
    op.add_column('sentiment_results', sa.Column('sentiment_scores_json', postgresql.JSONB(astext_type=sa.Text()), nullable=True, comment='JSON object of scores for all sentiment classes.'))
    op.alter_column('sentiment_results', 'event_id',
               existing_type=sa.BIGINT(),
               comment='Identifier of the original event. Logically references raw_events.id (BIGINT). Not an enforced FK due to TimescaleDB limitations.',
               existing_comment='The unique identifier of the event from its source.',
               existing_nullable=False)
    op.drop_index(op.f('idx_sentiment_result_processed_at'), table_name='sentiment_results')
    # ### end Alembic commands ###


def downgrade() -> None:
    """Revert the incremental upgrade.

    Drops the columns added in :pyfunc:`upgrade` **only if they exist**, leaving
    the previous schema intact.  It purposefully avoids touching any table
    other than `sentiment_results` and `sentiment_metrics`.
    """
    bind = op.get_bind()
    inspector = sa.inspect(bind)

    # sentiment_results: remove JSON score column
    sr_columns = {c["name"] for c in inspector.get_columns("sentiment_results")}
    if "sentiment_scores_json" in sr_columns:
        op.drop_column("sentiment_results", "sentiment_scores_json")

    # sentiment_metrics: drop added columns (if present)
    sm_columns = {c["name"] for c in inspector.get_columns("sentiment_metrics")}
    for col_name in [
        "avg_score",
        "count",
        "label",
        "source_id",
        "source",
        "time_bucket",
    ]:
        if col_name in sm_columns:
            op.drop_column("sentiment_metrics", col_name)

    # No attempt is made to change hypertable dimensions back; TimescaleDB will
    # ignore dropped dimensions for existing chunks, and new chunks will fall
    # back to the old one (metric_timestamp) if that column still exists.

    return  # prevent execution of legacy autogenerated commands below

    # ### commands auto generated by Alembic - please adjust! ###
    op.create_index(op.f('idx_sentiment_result_processed_at'), 'sentiment_results', ['processed_at'], unique=False)
    op.alter_column('sentiment_results', 'event_id',
               existing_type=sa.BIGINT(),
               comment='The unique identifier of the event from its source.',
               existing_comment='Identifier of the original event. Logically references raw_events.id (BIGINT). Not an enforced FK due to TimescaleDB limitations.',
               existing_nullable=False)
    op.drop_column('sentiment_results', 'sentiment_scores_json')
    op.add_column('sentiment_metrics', sa.Column('metric_value_int', sa.INTEGER(), autoincrement=False, nullable=True, comment='Integer value of the metric.'))
    op.add_column('sentiment_metrics', sa.Column('metric_name', sa.TEXT(), autoincrement=False, nullable=False, comment='Name of the metric (e.g., avg_sentiment_score).'))
    op.add_column('sentiment_metrics', sa.Column('tags', postgresql.JSONB(astext_type=sa.Text()), autoincrement=False, nullable=True, comment='Tags associated with the metric (e.g., source, model_version).'))
    op.add_column('sentiment_metrics', sa.Column('source_component', sa.TEXT(), autoincrement=False, nullable=True, comment='Component that generated the metric.'))
    op.add_column('sentiment_metrics', sa.Column('metric_timestamp', postgresql.TIMESTAMP(timezone=True), server_default=sa.text('CURRENT_TIMESTAMP'), autoincrement=False, nullable=False, comment='Timestamp for the metric aggregation window or event.'))
    op.add_column('sentiment_metrics', sa.Column('metric_value_json', postgresql.JSONB(astext_type=sa.Text()), autoincrement=False, nullable=True, comment='JSON value of the metric.'))
    op.add_column('sentiment_metrics', sa.Column('metric_value_float', sa.DOUBLE_PRECISION(precision=53), autoincrement=False, nullable=True, comment='Floating point value of the metric.'))
    op.create_index(op.f('sentiment_metrics_metric_timestamp_idx'), 'sentiment_metrics', [sa.literal_column('metric_timestamp DESC')], unique=False)
    op.create_index(op.f('idx_sm_tags'), 'sentiment_metrics', ['tags'], unique=False, postgresql_using='gin')
    op.create_index(op.f('idx_sm_source_component'), 'sentiment_metrics', ['source_component'], unique=False)
    op.drop_column('sentiment_metrics', 'avg_score')
    op.drop_column('sentiment_metrics', 'count')
    op.drop_column('sentiment_metrics', 'label')
    op.drop_column('sentiment_metrics', 'source_id')
    op.drop_column('sentiment_metrics', 'source')
    op.drop_column('sentiment_metrics', 'time_bucket')
    op.create_table('raw_events',
    sa.Column('id', sa.BIGINT(), sa.Identity(always=False, start=1, increment=1, minvalue=1, maxvalue=9223372036854775807, cycle=False, cache=1), autoincrement=True, nullable=False, comment='Auto-incrementing primary key'),
    sa.Column('source', sa.TEXT(), autoincrement=False, nullable=False, comment="Source system of the event (e.g., 'reddit', 'twitter')"),
    sa.Column('source_id', sa.TEXT(), autoincrement=False, nullable=False, comment='Unique identifier of the event within the source system'),
    sa.Column('occurred_at', postgresql.TIMESTAMP(timezone=True), autoincrement=False, nullable=False, comment='Timestamp when the event originally occurred'),
    sa.Column('payload', postgresql.JSONB(astext_type=sa.Text()), autoincrement=False, nullable=False, comment='Full event payload as JSON'),
    sa.Column('ingested_at', postgresql.TIMESTAMP(timezone=True), server_default=sa.text('now()'), autoincrement=False, nullable=False, comment='Timestamp when the event was ingested into the system'),
    sa.Column('processed', sa.BOOLEAN(), server_default=sa.text('false'), autoincrement=False, nullable=False, comment='Flag indicating if the event has been processed'),
    sa.Column('processed_at', postgresql.TIMESTAMP(timezone=True), autoincrement=False, nullable=True, comment='Timestamp when sentiment analysis was completed for this event'),
    sa.PrimaryKeyConstraint('id', 'occurred_at', name=op.f('pk_raw_events')),
    sa.UniqueConstraint('source', 'source_id', 'occurred_at', name=op.f('uq_raw_events_source_source_id_occurred_at'), postgresql_include=[], postgresql_nulls_not_distinct=False),
    comment='Stores raw event data from various sources. Partitioned by occurred_at.'
    )
    op.create_index(op.f('ix_raw_events_source_source_id'), 'raw_events', ['source', 'source_id'], unique=False)
    op.create_index(op.f('ix_raw_events_processed_occurred_at'), 'raw_events', ['processed', 'occurred_at'], unique=False, postgresql_where='(processed = false)')
    op.create_index(op.f('ix_raw_events_occurred_at'), 'raw_events', ['occurred_at'], unique=False)
    op.create_table('sentiment_scores',
    sa.Column('id', sa.INTEGER(), autoincrement=True, nullable=False),
    sa.Column('event_id', sa.INTEGER(), autoincrement=False, nullable=False),
    sa.Column('score', sa.INTEGER(), autoincrement=False, nullable=False),
    sa.Column('confidence', sa.INTEGER(), autoincrement=False, nullable=False),
    sa.Column('created_at', postgresql.TIMESTAMP(timezone=True), server_default=sa.text('now()'), autoincrement=False, nullable=False),
    sa.PrimaryKeyConstraint('id', name=op.f('sentiment_scores_pkey'))
    )
    # ### end Alembic commands ###
