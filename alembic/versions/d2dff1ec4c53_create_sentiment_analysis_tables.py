"""create_sentiment_analysis_tables

Revision ID: d2dff1ec4c53
Revises: 2dde641de514
Create Date: 2025-06-05 16:43:21.658538

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = 'd2dff1ec4c53'
down_revision: Union[str, None] = '2dde641de514'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


from sqlalchemy.dialects import postgresql

def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    # Manually defined migration steps

    # Create sentiment_results table
    op.create_table('sentiment_results',
        sa.Column('id', sa.BigInteger(), autoincrement=True, nullable=False, comment='Unique identifier for the sentiment result.'),
        sa.Column('event_id', sa.BigInteger(), nullable=False, comment="Identifier of the original event. Foreign key to raw_events.id (BIGINT)."),
        sa.Column('occurred_at', postgresql.TIMESTAMP(timezone=True), nullable=False, comment='Timestamp when the event originally occurred.'),
        sa.Column('source', sa.Text(), nullable=False, comment="The origin of the data (e.g., 'reddit')."),
        sa.Column('source_id', sa.Text(), nullable=False, comment='A secondary identifier from the source (e.g., subreddit name).'),
        sa.Column('sentiment_score', sa.Float(), nullable=False, comment='The calculated sentiment score.'),
        sa.Column('sentiment_label', sa.Text(), nullable=False, comment='The categorical sentiment label.'),
        sa.Column('confidence', sa.Float(), nullable=True, comment='Confidence level of the sentiment prediction.'),
        sa.Column('processed_at', postgresql.TIMESTAMP(timezone=True), server_default=sa.func.current_timestamp(), nullable=False, comment='Timestamp of sentiment processing.'),
        sa.Column('model_version', sa.Text(), nullable=False, comment='Version of the sentiment analysis model used.'),
        sa.Column('raw_text', sa.Text(), nullable=True, comment='The original text that was analyzed.'),
        sa.PrimaryKeyConstraint('id', 'processed_at', name='pk_sentiment_result'),
        sa.UniqueConstraint('event_id', 'occurred_at', 'processed_at', name='uq_sentiment_result_event_occurred_processed_at')
    )
    op.create_index('idx_sentiment_result_event_id_occurred_at', 'sentiment_results', ['event_id', 'occurred_at'], unique=False)
    op.create_index('idx_sentiment_result_label_time', 'sentiment_results', ['sentiment_label', 'occurred_at'], unique=False)
    op.create_index('idx_sentiment_result_src_time', 'sentiment_results', ['source', 'occurred_at'], unique=False)
    op.create_index('idx_sentiment_result_processed_at', 'sentiment_results', ['processed_at'], unique=False) # Index for hypertable

    # Convert sentiment_results to hypertable
    op.execute("SELECT create_hypertable('sentiment_results', 'processed_at', if_not_exists => TRUE);")

    # Create sentiment_metrics table
    op.create_table('sentiment_metrics',
        sa.Column('metric_name', sa.Text(), nullable=False, comment='Name of the metric (e.g., avg_sentiment_score).'),
        sa.Column('metric_timestamp', postgresql.TIMESTAMP(timezone=True), server_default=sa.func.current_timestamp(), nullable=False, comment='Timestamp for the metric aggregation window or event.'),
        sa.Column('metric_value_float', sa.Float(), nullable=True, comment='Floating point value of the metric.'),
        sa.Column('metric_value_int', sa.Integer(), nullable=True, comment='Integer value of the metric.'),
        sa.Column('metric_value_json', postgresql.JSONB(astext_type=sa.Text()), nullable=True, comment='JSON value of the metric.'),
        sa.Column('tags', postgresql.JSONB(astext_type=sa.Text()), nullable=True, comment='Tags associated with the metric (e.g., source, model_version).'),
        sa.Column('source_component', sa.Text(), nullable=True, comment='Component that generated the metric.'),
        sa.PrimaryKeyConstraint('metric_name', 'metric_timestamp')
    )
    op.create_index('idx_sm_source_component', 'sentiment_metrics', ['source_component'], unique=False)
    op.create_index('idx_sm_tags', 'sentiment_metrics', ['tags'], unique=False, postgresql_using='gin')
    # PK covers idx_sm_name_timestamp

    # Convert sentiment_metrics to hypertable
    op.execute("SELECT create_hypertable('sentiment_metrics', 'metric_timestamp', if_not_exists => TRUE);");

    # Create dead_letter_events table
    op.create_table('dead_letter_events',
        sa.Column('id', sa.Integer(), autoincrement=True, nullable=False, comment='Part of composite PK, auto-incrementing.'),
        sa.Column('event_id', sa.Text(), nullable=False, comment="Identifier of the original event."),
        sa.Column('occurred_at', postgresql.TIMESTAMP(timezone=True), nullable=False, comment='Timestamp of the original event.'),
        sa.Column('source', sa.Text(), nullable=False, comment='Source of the original event.'),
        sa.Column('source_id', sa.Text(), nullable=False, comment='Source-specific ID of the original event.'),
        sa.Column('event_payload', postgresql.JSONB(astext_type=sa.Text()), nullable=True, comment='Payload of the original event.'),
        sa.Column('processing_component', sa.Text(), nullable=True, comment='Component where processing failed.'),
        sa.Column('error_msg', sa.Text(), nullable=True, comment='Error message detailing the failure.'),
        sa.Column('failed_at', postgresql.TIMESTAMP(timezone=True), server_default=sa.func.current_timestamp(), nullable=False, comment='Timestamp of failure.'),
        sa.PrimaryKeyConstraint('id', 'failed_at', name='pk_dead_letter_event')
    )
    op.create_index('idx_dle_event_id_occurred_at', 'dead_letter_events', ['event_id', 'occurred_at'], unique=False)
    op.create_index('idx_dle_failed_at', 'dead_letter_events', ['failed_at'], unique=False)

    # Convert dead_letter_events to hypertable
    op.execute("SELECT create_hypertable('dead_letter_events', 'failed_at', if_not_exists => TRUE);");

    # Foreign key from sentiment_results (hypertable) to raw_events (hypertable)
    # is NOT created due to TimescaleDB limitations.
    # Referential integrity for (event_id, occurred_at) will be managed at the application level.

    # ### end Alembic commands ###

def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    # Manually defined migration steps

    # Drop tables in reverse order of creation
    # For hypertables, TimescaleDB handles associated chunks when the main table is dropped.
    op.drop_table('dead_letter_events')
    op.drop_table('sentiment_metrics')
    op.drop_table('sentiment_results')
    # ### end Alembic commands ###
